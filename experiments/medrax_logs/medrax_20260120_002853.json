HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "11583", "question_id": "11583_8353800736258", "timestamp": "2026-01-20T00:29:50.545971", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "11583", "question_id": "11583_8353800841102", "timestamp": "2026-01-20T00:29:53.608971", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "11583", "question_id": "11583_8353800778861", "timestamp": "2026-01-20T00:29:54.796819", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "11583", "question_id": "11583_8353800764059", "timestamp": "2026-01-20T00:29:55.919311", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "11583", "question_id": "11583_8353800840529", "timestamp": "2026-01-20T00:29:57.132844", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 400 Bad Request"
{"case_id": "13388", "question_id": "13388_8353800587274", "timestamp": "2026-01-20T00:29:58.523671", "model": "medrax", "status": "error", "error": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'The provided image url can not be accessed. status code: 403.', 'param': None, 'type': None}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 413 Payload Too Large"
{"case_id": "13388", "question_id": "13388_8353800651470", "timestamp": "2026-01-20T00:29:59.534729", "model": "medrax", "status": "error", "error": "Error code: 413 - {'error': {'code': 'tokens_limit_reached', 'message': 'Request body too large for gpt-4o model. Max size: 8000 tokens.', 'details': 'Request body too large for gpt-4o model. Max size: 8000 tokens.'}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 413 Payload Too Large"
{"case_id": "13388", "question_id": "13388_8353800704178", "timestamp": "2026-01-20T00:30:00.713645", "model": "medrax", "status": "error", "error": "Error code: 413 - {'error': {'code': 'tokens_limit_reached', 'message': 'Request body too large for gpt-4o model. Max size: 8000 tokens.', 'details': 'Request body too large for gpt-4o model. Max size: 8000 tokens.'}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 413 Payload Too Large"
{"case_id": "10098", "question_id": "10098_8353800802244", "timestamp": "2026-01-20T00:30:01.716389", "model": "medrax", "status": "error", "error": "Error code: 413 - {'error': {'code': 'tokens_limit_reached', 'message': 'Request body too large for gpt-4o model. Max size: 8000 tokens.', 'details': 'Request body too large for gpt-4o model. Max size: 8000 tokens.'}}"}
HTTP Request: POST https://models.inference.ai.azure.com/chat/completions "HTTP/1.1 413 Payload Too Large"
{"case_id": "10098", "question_id": "10098_8353800828916", "timestamp": "2026-01-20T00:30:02.346856", "model": "medrax", "status": "error", "error": "Error code: 413 - {'error': {'code': 'tokens_limit_reached', 'message': 'Request body too large for gpt-4o model. Max size: 8000 tokens.', 'details': 'Request body too large for gpt-4o model. Max size: 8000 tokens.'}}"}
