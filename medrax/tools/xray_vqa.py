from typing import Dict, List, Optional, Tuple, Type, Any, Union
from pathlib import Path
from pydantic import BaseModel, Field, field_validator
import json

import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_core.callbacks import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain_core.tools import BaseTool


class XRayVQAToolInput(BaseModel):
    """Input schema for the CheXagent Tool."""

    image_paths: Union[List[str], str] = Field(
        ..., description="List of paths to chest X-ray images to analyze"
    )
    prompt: str = Field(..., description="Question or instruction about the chest X-ray images")
    max_new_tokens: int = Field(
        512, description="Maximum number of tokens to generate in the response"
    )

    @field_validator('image_paths', mode='before')
    @classmethod
    def parse_image_paths(cls, v):
        """Parse image_paths if it's a string representation of a list."""
        if isinstance(v, str):
            # Remove leading/trailing brackets if present
            v = v.strip()
            if v.startswith('[') and v.endswith(']'):
                v = v[1:-1].strip()
            
            # Try to parse as JSON
            try:
                parsed = json.loads(v)
                if isinstance(parsed, list):
                    return parsed
            except json.JSONDecodeError:
                pass
            
            # Check if it looks like a comma-separated list
            if ',' in v:
                paths = [p.strip().strip('"').strip("'") for p in v.split(',')]
                return [p for p in paths if p]  # Remove empty strings
            
            # If single path, wrap in list
            return [v]
        return v


class XRayVQATool(BaseTool):
    """Tool that leverages CheXagent for comprehensive chest X-ray analysis."""

    name: str = "chest_xray_expert"
    description: str = (
        "A versatile tool for analyzing chest X-rays. "
        "Can perform multiple tasks including: visual question answering, report generation, "
        "abnormality detection, comparative analysis, anatomical description, "
        "and clinical interpretation. Input should be paths to X-ray images "
        "and a natural language prompt describing the analysis needed."
    )
    args_schema: Type[BaseModel] = XRayVQAToolInput
    return_direct: bool = True
    cache_dir: Optional[str] = None
    device: Optional[str] = None
    dtype: torch.dtype = torch.bfloat16
    tokenizer: Optional[AutoTokenizer] = None
    model: Optional[AutoModelForCausalLM] = None

    def __init__(
        self,
        model_name: str = "StanfordAIMI/CheXagent-2-3b",
        device: Optional[str] = "cuda",
        dtype: torch.dtype = torch.bfloat16,
        cache_dir: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """Initialize the XRayVQATool.

        Args:
            model_name: Name of the CheXagent model to use
            device: Device to run model on (cuda/cpu)
            dtype: Data type for model weights
            cache_dir: Directory to cache downloaded models
            **kwargs: Additional arguments
        """
        super().__init__(**kwargs)

        # Dangerous code, but works for now
        import transformers

        original_transformers_version = transformers.__version__
        transformers.__version__ = "4.40.0"

        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = dtype
        self.cache_dir = cache_dir

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            cache_dir=cache_dir,
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=self.device,
            trust_remote_code=True,            cache_dir=cache_dir,
        )
        self.model = self.model.to(dtype=self.dtype)
        self.model.eval()

        transformers.__version__ = original_transformers_version

    def _generate_response(self, image_paths: List[str], prompt: str, max_new_tokens: int,
                            do_sample: bool = False, temperature: float = 1.0, top_p: float = 1.0) -> str:
        """Generate response using CheXagent model.

        Args:
            image_paths: List of paths to chest X-ray images
            prompt: Question or instruction about the images
            max_new_tokens: Maximum number of tokens to generate
            do_sample: Whether to use sampling (for self-consistency)
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
        Returns:
            str: Model's response
        """
        query = self.tokenizer.from_list_format(
            [*[{"image": path} for path in image_paths], {"text": prompt}]
        )
        conv = [
            {"from": "system", "value": "You are a helpful assistant."},
            {"from": "human", "value": query},
        ]
        input_ids = self.tokenizer.apply_chat_template(
            conv, add_generation_prompt=True, return_tensors="pt"
        ).to(device=self.device)

        # Run inference
        with torch.inference_mode():
            output = self.model.generate(
                input_ids,
                do_sample=do_sample,
                num_beams=1,
                temperature=temperature,
                top_p=top_p,
                use_cache=True,
                max_new_tokens=max_new_tokens,
            )[0]
            response = self.tokenizer.decode(output[input_ids.size(1) : -1])

            return response

    def _generate_samples_for_confidence(self, image_paths: List[str], prompt: str, 
                                          max_new_tokens: int, num_samples: int = 5,
                                          temperature: float = 0.7) -> List[str]:
        """Generate multiple samples for self-consistency confidence scoring.
        
        Args:
            image_paths: List of paths to chest X-ray images
            prompt: Question or instruction about the images
            max_new_tokens: Maximum number of tokens to generate
            num_samples: Number of samples to generate
            temperature: Sampling temperature for diversity
            
        Returns:
            List[str]: List of generated responses
        """
        samples = []
        for _ in range(num_samples):
            response = self._generate_response(
                image_paths, prompt, max_new_tokens,
                do_sample=True, temperature=temperature, top_p=0.9
            )
            samples.append(response)
        return samples

    def _compute_self_consistency_score(self, samples: List[str]) -> Dict[str, Any]:
        """Compute self-consistency confidence score from multiple samples.
        
        Args:
            samples: List of generated responses
            
        Returns:
            Dict with confidence metrics
        """
        from collections import Counter
        
        # Normalize samples for comparison (lowercase, strip whitespace)
        normalized = [s.strip().lower() for s in samples]
        
        # Count frequency of each unique answer
        counter = Counter(normalized)
        most_common_answer, most_common_count = counter.most_common(1)[0]
        
        # Self-consistency score = frequency of most common answer / total samples
        consistency_score = most_common_count / len(samples)
        
        # Find the original (non-normalized) most common answer
        for s in samples:
            if s.strip().lower() == most_common_answer:
                most_common_original = s
                break
        
        return {
            "consistency_score": consistency_score,
            "num_samples": len(samples),
            "num_unique_answers": len(counter),
            "most_common_count": most_common_count,
            "answer_distribution": dict(counter),
            "consensus_answer": most_common_original,
        }

    def _run(
        self,
        image_paths: List[str],
        prompt: str,
        max_new_tokens: int = 512,
        run_manager: Optional[CallbackManagerForToolRun] = None,
    ) -> Tuple[Dict[str, Any], Dict]:
        """Execute the chest X-ray analysis.

        Args:
            image_paths: List of paths to chest X-ray images
            prompt: Question or instruction about the images
            max_new_tokens: Maximum number of tokens to generate
            run_manager: Optional callback manager

        Returns:
            Tuple[Dict[str, Any], Dict]: Output dictionary and metadata dictionary
        """
        try:
            # Verify image paths
            for path in image_paths:
                if not Path(path).is_file():
                    raise FileNotFoundError(f"Image file not found: {path}")

            # Generate primary response (deterministic)
            response = self._generate_response(image_paths, prompt, max_new_tokens)
            
            # Generate multiple samples for self-consistency confidence
            samples = self._generate_samples_for_confidence(
                image_paths, prompt, max_new_tokens, num_samples=5, temperature=0.7
            )
            
            # Compute self-consistency confidence score
            confidence_data = self._compute_self_consistency_score(samples)

            output = {
                "response": response,
            }

            metadata = {
                "image_paths": image_paths,
                "prompt": prompt,
                "max_new_tokens": max_new_tokens,
                "analysis_status": "completed",
                # Confidence-enabling data for self-consistency scoring
                "confidence_data": {
                    "samples": samples,
                    "consistency_score": confidence_data["consistency_score"],
                    "num_unique_answers": confidence_data["num_unique_answers"],
                    "answer_distribution": confidence_data["answer_distribution"],
                    "consensus_answer": confidence_data["consensus_answer"],
                },
            }

            return output, metadata

        except Exception as e:
            output = {"error": str(e)}
            metadata = {
                "image_paths": image_paths,
                "prompt": prompt,
                "max_new_tokens": max_new_tokens,
                "analysis_status": "failed",
                "error_details": str(e),
            }
            return output, metadata

    async def _arun(
        self,
        image_paths: List[str],
        prompt: str,
        max_new_tokens: int = 512,
        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
    ) -> Tuple[Dict[str, Any], Dict]:
        """Async version of _run."""
        return self._run(image_paths, prompt, max_new_tokens)
